{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b31ff9c-21da-4bca-9c72-5fb0bbd46fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/fm0yr04d2tz77p6w1vl9y72w0000gn/T/ipykernel_8608/2809025955.py:2: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"datasets/youtube_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/youtube_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67281362-4caa-4fe8-9173-fd81c6e625cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/harrietmathew/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER to raw text column\n",
    "df['vader_score'] = df['text'].astype(str).apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Classify sentiment based on compound score\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment'] = df['vader_score'].apply(classify_sentiment)\n",
    "\n",
    "# Convert datetime and extract date for trend analysis\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "df['date'] = df['datetime'].dt.date\n",
    "\n",
    "df.to_csv('datasets/youtube_with_sentiment.csv', index=False)\n",
    "print(\"youtube_with_sentiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23614f0c-d487-47e0-ab54-6518e7866683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Platform      Event  Avg. Sentiment  % Strong Positive (≥ 0.6)  \\\n",
      "0  YouTube   Olympics        0.068048                       13.5   \n",
      "1  YouTube  World Cup        0.085426                       10.6   \n",
      "\n",
      "   % Strong Negative (≤ -0.6)  \n",
      "0                         7.0  \n",
      "1                         2.4  \n"
     ]
    }
   ],
   "source": [
    "# Classify strong sentiment\n",
    "def classify_strong_sentiment(score):\n",
    "    if score >= 0.6:\n",
    "        return 'strong_positive'\n",
    "    elif score <= -0.6:\n",
    "        return 'strong_negative'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df['platform'] = 'YouTube'\n",
    "df['strong_sentiment'] = df['vader_score'].apply(classify_strong_sentiment)\n",
    "\n",
    "# Group and summarize\n",
    "grouped = df.groupby(['platform', 'event'])\n",
    "summary = grouped.agg(\n",
    "    avg_sentiment=('vader_score', 'mean'),\n",
    "    total=('vader_score', 'count'),\n",
    "    strong_positive=('strong_sentiment', lambda x: (x == 'strong_positive').sum()),\n",
    "    strong_negative=('strong_sentiment', lambda x: (x == 'strong_negative').sum())\n",
    ").reset_index()\n",
    "\n",
    "# Add percentage columns\n",
    "summary['% Strong Positive (≥ 0.6)'] = (summary['strong_positive'] / summary['total'] * 100).round(1)\n",
    "summary['% Strong Negative (≤ -0.6)'] = (summary['strong_negative'] / summary['total'] * 100).round(1)\n",
    "\n",
    "# Final summary table\n",
    "summary_table = summary[['platform', 'event', 'avg_sentiment', '% Strong Positive (≥ 0.6)', '% Strong Negative (≤ -0.6)']]\n",
    "summary_table.columns = ['Platform', 'Event', 'Avg. Sentiment', '% Strong Positive (≥ 0.6)', '% Strong Negative (≤ -0.6)']\n",
    "\n",
    "# Save the summary table\n",
    "summary_table.to_csv('datasets/youtube_emotional_unity_summary.csv', index=False)\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab7251e3-31ae-4d23-a963-6a97bd93d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Platform      Event  Avg. Sentiment  % Strong Positive (≥ 0.6)  \\\n",
      "0   Reddit   Olympics        0.115646                       18.2   \n",
      "1   Reddit  World Cup        0.083151                       15.8   \n",
      "2  YouTube   Olympics        0.068048                       13.5   \n",
      "3  YouTube  World Cup        0.085426                       10.6   \n",
      "\n",
      "   % Strong Negative (≤ -0.6)  \n",
      "0                         7.8  \n",
      "1                         8.2  \n",
      "2                         7.0  \n",
      "3                         2.4  \n"
     ]
    }
   ],
   "source": [
    "reddit_summary = pd.read_csv('datasets/reddit_emotional_unity_summary.csv')\n",
    "youtube_summary = pd.read_csv('datasets/youtube_emotional_unity_summary.csv')\n",
    "\n",
    "combined_summary = pd.concat([reddit_summary, youtube_summary], ignore_index=True)\n",
    "print(combined_summary)\n",
    "\n",
    "combined_summary.to_csv('datasets/emotional_unity_combined_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc07512-eb79-4333-abe9-55c375271712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/fm0yr04d2tz77p6w1vl9y72w0000gn/T/ipykernel_8608/1559239527.py:6: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('datasets/youtube_with_sentiment.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Topics in Strong Positive Comments - World Cup\n",
      "Topic 1: win, croatia, like, team, france, year, country, respect, dont, know\n",
      "Topic 2: best, world, cup, messi, goal, argentina, final, fifa, player, win\n",
      "Topic 3: great, goal, russia, amazing, beautiful, yes, brazil, football, greatest, time\n",
      "Topic 4: love, song, jungkook, like, life, bts, live, proud, masterpiece, wow\n",
      "\n",
      "Top Topics in Strong Negative Comments - World Cup\n",
      "Topic 1: team, fan, game, penalty, bad, didnt, match, ronaldo, final, stupid\n",
      "Topic 2: world, rigged, cup, goal, messi, penalty, france, argentina, lost, time\n",
      "Topic 3: country, penalty, racist, dead, win, song, fifa, got, dont, say\n",
      "Topic 4: world, cup, worst, que, song, russia, war, final, country, qatar\n",
      "\n",
      "Top Topics in Strong Positive Comments - Olympics\n",
      "Topic 1: like, people, god, dont, jesus, olympics, sport, world, know, thing\n",
      "Topic 2: best, like, olympics, medal, time, gold, game, olympic, world, team\n",
      "Topic 3: love, great, like, beautiful, proud, win, race, congratulation, world, awesome\n",
      "Topic 4: god, wow, thank, people, amazing, celine, performance, japan, greek, france\n",
      "\n",
      "Top Topics in Strong Negative Comments - Olympics\n",
      "Topic 1: worst, opening, ceremony, olympic, olympics, woman, disgusting, shame, history, game\n",
      "Topic 2: olympics, dont, hell, people, thing, disgusting, seen, paris, like, hate\n",
      "Topic 3: god, jesus, people, christian, world, christ, bad, know, evil, lord\n",
      "Topic 4: people, country, france, christian, french, olympics, world, supper, like, religion\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load preprocessed dataset\n",
    "df = pd.read_csv('datasets/youtube_with_sentiment.csv')\n",
    "\n",
    "# Use already cleaned text for LDA\n",
    "df['lda_text'] = df['clean_text']\n",
    "df['strong_sentiment'] = df['vader_score'].apply(classify_strong_sentiment)\n",
    "\n",
    "# LDA topic function\n",
    "def get_topics_from_texts(texts, n_topics=4, n_top_words=10):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topic_keywords.append((f\"Topic {topic_idx + 1}\", top_words))\n",
    "    return topic_keywords\n",
    "\n",
    "for event in ['World Cup', 'Olympics']:\n",
    "    for sentiment in ['strong_positive', 'strong_negative']:\n",
    "        subset = df[(df['event'] == event) & (df['strong_sentiment'] == sentiment)]\n",
    "        subset = subset.dropna(subset=['lda_text'])\n",
    "        if not subset.empty:\n",
    "            print(f\"\\nTop Topics in {sentiment.replace('_', ' ').title()} Comments - {event}\")\n",
    "            topics = get_topics_from_texts(subset['lda_text'])\n",
    "            for topic, keywords in topics:\n",
    "                print(f\"{topic}: {', '.join(keywords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203e7e1-2b12-489c-b26a-09987954f6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
