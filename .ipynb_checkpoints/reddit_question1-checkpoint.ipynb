{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b31ff9c-21da-4bca-9c72-5fb0bbd46fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/reddit_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67281362-4caa-4fe8-9173-fd81c6e625cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/harrietmathew/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER to raw text column\n",
    "df['vader_score'] = df['text'].astype(str).apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Classify sentiment based on compound score\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment'] = df['vader_score'].apply(classify_sentiment)\n",
    "\n",
    "# Convert datetime and extract date for trend analysis\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "df['date'] = df['datetime'].dt.date\n",
    "\n",
    "df.to_csv('datasets/reddit_with_sentiment.csv', index=False)\n",
    "print(\"reddit_with_sentiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23614f0c-d487-47e0-ab54-6518e7866683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Platform      Event  Avg. Sentiment  % Strong Positive (≥ 0.6)  \\\n",
      "0   Reddit   Olympics        0.142080                       22.7   \n",
      "1   Reddit  World Cup        0.120668                       22.0   \n",
      "\n",
      "   % Strong Negative (≤ -0.6)  \n",
      "0                         8.8  \n",
      "1                         9.5  \n"
     ]
    }
   ],
   "source": [
    "# Classify strong sentiment\n",
    "def classify_strong_sentiment(score):\n",
    "    if score >= 0.6:\n",
    "        return 'strong_positive'\n",
    "    elif score <= -0.6:\n",
    "        return 'strong_negative'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df['platform'] = 'Reddit' \n",
    "df['strong_sentiment'] = df['vader_score'].apply(classify_strong_sentiment)\n",
    "\n",
    "# Group and summarize\n",
    "grouped = df.groupby(['platform', 'event'])\n",
    "summary = grouped.agg(\n",
    "    avg_sentiment=('vader_score', 'mean'),\n",
    "    total=('vader_score', 'count'),\n",
    "    strong_positive=('strong_sentiment', lambda x: (x == 'strong_positive').sum()),\n",
    "    strong_negative=('strong_sentiment', lambda x: (x == 'strong_negative').sum())\n",
    ").reset_index()\n",
    "\n",
    "# Add percentage columns\n",
    "summary['% Strong Positive (≥ 0.6)'] = (summary['strong_positive'] / summary['total'] * 100).round(1)\n",
    "summary['% Strong Negative (≤ -0.6)'] = (summary['strong_negative'] / summary['total'] * 100).round(1)\n",
    "\n",
    "# Final summary table\n",
    "summary_table = summary[['platform', 'event', 'avg_sentiment', '% Strong Positive (≥ 0.6)', '% Strong Negative (≤ -0.6)']]\n",
    "summary_table.columns = ['Platform', 'Event', 'Avg. Sentiment', '% Strong Positive (≥ 0.6)', '% Strong Negative (≤ -0.6)']\n",
    "\n",
    "# Save table\n",
    "summary_table.to_csv('datasets/reddit_emotional_unity_summary.csv', index=False)\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ddab0b-6381-4912-9690-2a6baf4abfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Platform      Event  Avg. Sentiment  % Strong Positive (≥ 0.6)  \\\n",
      "0   Reddit   Olympics        0.142080                       22.7   \n",
      "1   Reddit  World Cup        0.120668                       22.0   \n",
      "2  YouTube   Olympics        0.060340                       14.9   \n",
      "3  YouTube  World Cup        0.073496                       13.2   \n",
      "\n",
      "   % Strong Negative (≤ -0.6)  \n",
      "0                         8.8  \n",
      "1                         9.5  \n",
      "2                         9.3  \n",
      "3                         5.6  \n"
     ]
    }
   ],
   "source": [
    "reddit_summary = pd.read_csv('datasets/reddit_emotional_unity_summary.csv')\n",
    "youtube_summary = pd.read_csv('datasets/youtube_emotional_unity_summary.csv')\n",
    "\n",
    "combined_summary = pd.concat([reddit_summary, youtube_summary], ignore_index=True)\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91952414-577f-436b-9a59-a436930480ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Topics in Strong Positive Comments - World Cup\n",
      "Topic 1: thank, like, great, post, russia, love, thanks, question, qatar, people\n",
      "Topic 2: like, team, year, post, time, race, point, make, sport, championship\n",
      "Topic 3: good, really, like, game, happy, love, think, people, year, time\n",
      "Topic 4: team, world, cup, player, game, win, best, final, like, play\n",
      "\n",
      "Top Topics in Strong Negative Comments - World Cup\n",
      "Topic 1: like, game, dont, time, people, team, shit, hell, going, really\n",
      "Topic 2: world, cup, russia, country, fifa, team, russian, people, shit, war\n",
      "Topic 3: day, group, thing, like, dead, cheating, let, damn, new, evidence\n",
      "Topic 4: qatar, people, fuck, world, dont, cup, like, make, country, year\n",
      "\n",
      "Top Topics in Strong Positive Comments - Olympics\n",
      "Topic 1: like, love, great, make, dont, really, know, people, thing, good\n",
      "Topic 2: song, like, love, album, really, good, feel, dont, track, sound\n",
      "Topic 3: like, olympics, medal, time, gold, year, going, think, people, best\n",
      "Topic 4: like, game, really, good, people, amazing, year, best, think, pretty\n",
      "\n",
      "Top Topics in Strong Negative Comments - Olympics\n",
      "Topic 1: dick, like, family, people, trump, time, right, pound, dont, brother\n",
      "Topic 2: trump, people, dont, know, really, right, thing, match, country, year\n",
      "Topic 3: people, woman, like, think, dont, bad, say, thing, hate, really\n",
      "Topic 4: olympics, fuck, shit, game, people, bad, going, hell, time, think\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load preprocessed dataset\n",
    "df = pd.read_csv('datasets/reddit_with_sentiment.csv')\n",
    "\n",
    "# Use already cleaned text for LDA\n",
    "df['lda_text'] = df['clean_text']\n",
    "df['strong_sentiment'] = df['vader_score'].apply(classify_strong_sentiment)\n",
    "\n",
    "# LDA topic function\n",
    "def get_topics_from_texts(texts, n_topics=4, n_top_words=10):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topic_keywords.append((f\"Topic {topic_idx + 1}\", top_words))\n",
    "    return topic_keywords\n",
    "    \n",
    "subset = subset.dropna(subset=['lda_text']) \n",
    "\n",
    "for event in ['World Cup', 'Olympics']:\n",
    "    for sentiment in ['strong_positive', 'strong_negative']:\n",
    "        subset = df[(df['event'] == event) & (df['strong_sentiment'] == sentiment)]\n",
    "        if not subset.empty:\n",
    "            print(f\"\\nTop Topics in {sentiment.replace('_', ' ').title()} Comments - {event}\")\n",
    "            topics = get_topics_from_texts(subset['lda_text'])\n",
    "            for topic, keywords in topics:\n",
    "                print(f\"{topic}: {', '.join(keywords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef8f4f-1270-4432-b559-8bae55ecb59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
